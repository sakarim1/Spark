{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Spark Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Stack Exchange provides an anonymized data dump (https://archive.org/details/stackexchange), and I'll use Spark to perform data manipulation, analysis, and machine learning on this data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Data input and parsing - bad XML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First way - testing the logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109522"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('spark-stats-data/allPosts/').filter(lambda x: '<row'in x).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = sc.textFile('spark-stats-data/allPosts/').filter(lambda x: '<row'in x).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Body': '<p>Briefly, my answer is \"yes\".</p>\\n\\n<p>The result of any LDA inference algorithm is $\\\\theta_{d,t}$ and $\\\\phi_{t,w}$, distribution of topics in each document and distribution of terms in each topics. Given these distributions, one can obtain estimate for $p(z|d,w)$, conditional probability of a topic $z$ for word $w$ in document $d$:\\n$$\\np(z|d,w)=\\\\frac{p(z,d,w)}{\\\\sum\\\\limits_{s=1}^K p(s,d,w)}=\\\\frac{\\\\theta_{d,z} \\\\phi_{z,w}}{\\\\sum\\\\limits_{s=1}^{K} \\\\theta_{d, s} \\\\phi_{s, w}}\\n$$\\nFurther utilizing of this information can be, for example, assigning the single most probable topic for a word. But it\\'s not obligatory.</p>\\n',\n",
       " 'CommentCount': '0',\n",
       " 'CreationDate': '2013-06-15T09:36:39.560',\n",
       " 'Id': '61814',\n",
       " 'LastActivityDate': '2013-06-15T09:36:39.560',\n",
       " 'OwnerUserId': '26924',\n",
       " 'ParentId': '61648',\n",
       " 'PostTypeId': '2',\n",
       " 'Score': '2'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ET.fromstring(posts[7]).attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  <row Body=\"Autocorrelation is the correlation of a series of data with itself at some lag. This is an important topic particularly in the analysis of time-series data.\" CommentCount=\"0\" CreationDate=\"2013-06-16T20:13:54.233\" Id=\"61881\" LastActivityDate=\"2013-06-16T22:08:32.437\" LastEditDate=\"2013-06-16T22:08:32.437\" LastEditorUserId=\"7290\" OwnerUserId=\"7290\" PostTypeId=\"4\" Score=\"0\" />'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parser function for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_list(x):\n",
    "    try:\n",
    "        return dict(ET.fromstring(x).attrib)\n",
    "    except:\n",
    "        ET.ParseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(map(parser_list, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'AcceptedAnswerId': '61811',\n",
       "  'AnswerCount': '2',\n",
       "  'Body': '<p><strong>The situation</strong></p>\\n\\n<p>I have a dataset with one dependent $y$ and one independent variable $x$. I want to fit a continuous piecewise linear regression with $k$ known/fixed breakpoints occurring at $(a_{1}, a_{2}, \\\\ldots, a_{k})$. The breakpoins are known without uncertainty, so I don\\'t want to estimate them. Then I fit a regression (OLS) of the form\\n$$\\ny_{i} = \\\\beta_{0} + \\\\beta_{1}x_{i} + \\\\beta_{2}\\\\operatorname{max}(x_{i}-a_{1},0) +  \\\\beta_{3}\\\\operatorname{max}(x_{i}-a_{2},0) +\\\\ldots+ \\\\beta_{k+1}\\\\operatorname{max}(x_{i}-a_{k},0) +\\\\epsilon_{i}\\n$$\\nHere is an example in <code>R</code></p>\\n\\n<pre><code>set.seed(123)\\nx &lt;- c(1:10, 13:22)\\ny &lt;- numeric(20)\\ny[1:10] &lt;- 20:11 + rnorm(10, 0, 1.5)\\ny[11:20] &lt;- seq(11, 15, len=10) + rnorm(10, 0, 2)\\n</code></pre>\\n\\n<p>Let\\'s assume that the breakpoint $k_1$ occurs at $9.6$:</p>\\n\\n<pre><code>mod &lt;- lm(y~x+I(pmax(x-9.6, 0)))\\nsummary(mod)\\n\\nCoefficients:\\n                    Estimate Std. Error t value Pr(&gt;|t|)    \\n(Intercept)          21.7057     1.1726  18.511 1.06e-12 ***\\nx                    -1.1003     0.1788  -6.155 1.06e-05 ***\\nI(pmax(x - 9.6, 0))   1.3760     0.2688   5.120 8.54e-05 ***\\n---\\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\\n</code></pre>\\n\\n<p>The intercept and slope of the two segments are: $21.7$ and $-1.1$ for the first and $8.5$ and $0.27$ for the second, respectively.</p>\\n\\n<p><img src=\"http://i.stack.imgur.com/2GsoE.png\" alt=\"Breakpoint\"></p>\\n\\n<hr>\\n\\n<p><strong>Questions</strong></p>\\n\\n<ol>\\n<li>How to easily calculate the intercept and slope of each segment? Can the model be reparemetrized to do this in one calculation?</li>\\n<li>How to calculate the standard error of each slope of each segment?</li>\\n<li>How to test whether two adjacent slopes have the same slopes (i.e. whether the breakpoint can be omitted)?</li>\\n</ol>\\n',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T06:18:20.483',\n",
       "  'FavoriteCount': '1',\n",
       "  'Id': '61805',\n",
       "  'LastActivityDate': '2014-01-03T16:35:39.430',\n",
       "  'LastEditDate': '2014-01-03T16:35:39.430',\n",
       "  'LastEditorUserId': '805',\n",
       "  'OwnerUserId': '21054',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '6',\n",
       "  'Tags': '<r><regression><standard-error><piecewise-linear>',\n",
       "  'Title': 'Standard error of slopes in piecewise linear regression with known breakpoints',\n",
       "  'ViewCount': '1200'},\n",
       " {'Body': '<p>\"Better\" is a function of your model. </p>\\n\\n<p>Part of the reason for your confusion is you only wrote half of your model.</p>\\n\\n<p>When you say $y=ax^b$, that\\'s not actually true. Your observed $y$ values <em>aren\\'t</em> equal to $ax^b$; they have an error component. </p>\\n\\n<p>For example, the two models you mention (not the only possible models by any means) make entirely different assumptions about the error.</p>\\n\\n<p>You probably mean something closer to </p>\\n\\n<p>$E(Y|X=x) = ax^b\\\\,,$ or in a designed experiment, $E(Y|x) = ax^b$</p>\\n\\n<p>But what about the variation in $Y$ at a given $x$?</p>\\n\\n<ul>\\n<li>When you fit the nonlinear least squares model, you\\'re saying that the errors are additive and the standard deviation of the errors is constant across the data:</li>\\n</ul>\\n\\n<p>$y_i \\\\sim N(ax_i^b,\\\\sigma^2)$</p>\\n\\n<p>or equivalently</p>\\n\\n<p>$y_i = ax_i^b + e_i$, with $\\\\text{var}(e_i) = \\\\sigma^2$</p>\\n\\n<ul>\\n<li>by contrast when you take logs and fit a linear model, you\\'re saying the error is additive on the log scale and (on the log scale) constant across the data. This means that on the scale of the observations, the error term is <em>multiplicative</em>, and so the errors are larger when the expected values are larger:</li>\\n</ul>\\n\\n<p>$y_i \\\\sim \\\\text{logN}(\\\\log a+b\\\\log x_i,\\\\sigma^2)$</p>\\n\\n<p>or equivalently</p>\\n\\n<p>$y_i = ax_i^b \\\\cdot \\\\eta_i$, with $\\\\eta_i \\\\sim \\\\text{logN}(0,\\\\sigma^2)$</p>\\n\\n<p>(Note that $\\\\text{E}(\\\\eta)$ is not 1. If $\\\\sigma^2$ is small, you need to allow for this effect)</p>\\n\\n<p>(You can do least squares without assuming normality / lognormal distributions, but the central issue being discussed still applies ... and if you\\'re nowhere near normality, you should probably be considering a different error model anyway)</p>\\n\\n<p>So what is best depends on which one describes your circumstances. What does your data look like? What do the residuals look like against $x$?</p>\\n',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T06:44:59.437',\n",
       "  'Id': '61806',\n",
       "  'LastActivityDate': '2013-06-15T23:53:55.217',\n",
       "  'LastEditDate': '2013-06-15T23:53:55.217',\n",
       "  'LastEditorUserId': '805',\n",
       "  'OwnerUserId': '805',\n",
       "  'ParentId': '61747',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '7'},\n",
       " {'AnswerCount': '1',\n",
       "  'Body': \"<p>I've run a 2 (treatment and no treatment) x 2 testing occasions (pre and post) Repeated Measures ANOVA. In addition, I entered several tests given at both time points, so under the Repeated Measures Define Factor(s) window option for Measure Name, I've entered 5 different tests used. </p>\\n\\n<p>In the output, I am trying to determine which table indicates the main effect <em>across</em> all tests. What I am seeing instead is a Tests of Within-Subjects Effects Multivariate table (that I am ignoring) and a Univariate table that lists <code>session</code>, <code>session*group</code>, and <code>error(session)</code> effects by test. </p>\\n\\n<p>To report a main effect to address the question of whether <em>any</em> tests differed between the groups from pre to post, where would I find that information?</p>\\n\",\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T07:31:03.950',\n",
       "  'Id': '61808',\n",
       "  'LastActivityDate': '2013-06-16T01:59:40.520',\n",
       "  'LastEditDate': '2013-06-16T01:59:40.520',\n",
       "  'LastEditorUserId': '3826',\n",
       "  'OwnerUserId': '22293',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '2',\n",
       "  'Tags': '<anova><spss><repeated-measures><interpretation>',\n",
       "  'Title': 'Repeated Measures ANOVA with several measures entered together: how to find main effect',\n",
       "  'ViewCount': '325'},\n",
       " {'AnswerCount': '2',\n",
       "  'Body': '<p>In psychology and other fields a form of stepwise regression is often employed that involves the following: </p>\\n\\n<ol>\\n<li>Look at remaining predictors (there are none in the model at first) and identify the predictor that results in the largest r-square change; </li>\\n<li>If the p-value of the r-square change is less than alpha (typically .05), then include that predictor and go back to step 1, otherwise stop.</li>\\n</ol>\\n\\n<p>For example, see this procedure in <a href=\"http://www.geog.leeds.ac.uk/courses/other/statistics/spss/stepwise/\" rel=\"nofollow\">SPSS</a>.</p>\\n\\n<p>The procedure is routinely critiqued for a wide range of reasons (see <a href=\"http://www.stata.com/support/faqs/statistics/stepwise-regression-problems/\" rel=\"nofollow\">this discussion on the Stata website with references</a>). </p>\\n\\n<p>In particular, the Stata website summarises several comments by Frank Harrell. I\\'m interested in the claim:</p>\\n\\n<blockquote>\\n  <p>[stepwise regression] yields R-squared values that are badly biased to be high.</p>\\n</blockquote>\\n\\n<p>Specifically, some of my current research focuses on estimating <a href=\"http://stats.stackexchange.com/questions/55929/what-is-an-unbiased-estimate-of-population-r-square\">population r-square</a>. By population r-square I refer to the percentage of variance explained by the population data generating equation in the population. Much of the existing literature I am reviewing has used stepwise regression procedures and I want to know whether the estimates provided are biased and if so by how much. In particular, a typical study would have 30 predictors, n = 200, alpha of entry of .05, and r-square estimates around .50.</p>\\n\\n<p>What I do know:</p>\\n\\n<ul>\\n<li>Asymptotically, any predictor with a non-zero coefficient would be a statistically significant predictor, and r-square would equal adjusted r-square. Thus, asymptotically stepwise regression should estimate the true regression equation and the true population r-square.</li>\\n<li>With smaller sample sizes, the possible omission of some predictors will result in a smaller r-square than had all predictors been included in the model. But also the usual bias of r-square to sample data would increase the r-square. Thus, my naive thought is that potentially, these two opposing forces could under certain conditions result in an unbiased r-square. And more generally, the direction of the bias would be contingent on various features of the data and the alpha inclusion criteria.</li>\\n<li>Setting a more stringent alpha inclusion criterion (e.g., .01, .001, etc.) should lower expected estimated r-square because the probability of including any predictor in any generation of the data will be less.</li>\\n<li>In general, r-square is an upwardly biased estimate of population r-square and the degree of this bias increases with more predictors and smaller sample sizes.</li>\\n</ul>\\n\\n<h3>Question</h3>\\n\\n<p>So finally, my question:</p>\\n\\n<ul>\\n<li><strong>To what extent does the r-square from stepwise regression result in a biased estimate of population r-square?</strong></li>\\n<li><strong>To what extent is this bias related to sample size, number of predictors, alpha inclusion criterion or properties of the data?</strong></li>\\n<li><strong>Are there any references on this topic?</strong></li>\\n</ul>\\n',\n",
       "  'CommentCount': '7',\n",
       "  'CreationDate': '2013-06-15T07:42:51.720',\n",
       "  'FavoriteCount': '3',\n",
       "  'Id': '61809',\n",
       "  'LastActivityDate': '2013-07-05T00:50:39.337',\n",
       "  'LastEditDate': '2013-06-15T10:05:50.103',\n",
       "  'LastEditorUserId': '183',\n",
       "  'OwnerUserId': '183',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '10',\n",
       "  'Tags': '<regression><model-selection><r-squared><bias>',\n",
       "  'Title': 'Does stepwise regression provide a biased estimate of population r-square?',\n",
       "  'ViewCount': '602'},\n",
       " {'Body': \"<blockquote>\\n  <ol>\\n  <li>How to easily calculate the intercept and slope of each segment? </li>\\n  </ol>\\n</blockquote>\\n\\n<p>The slope of each segment is calculated by simply adding all the coefficients up to the current position. So the slope estimate at $x=15$ is   $-1.1003 + 1.3760  = 0.2757\\\\,$.</p>\\n\\n<p>The intercept is a little harder, but it's a linear combination of coefficients (involving the knots). </p>\\n\\n<p>In your example, the second line meets the first at $x=9.6$, so the red point is on the first line at $21.7057 -1.1003 \\\\times 9.6 = 11.1428$. Since the second line passes through the point $(9.6, 11.428)$ with slope $0.2757$, its intercept is $11.1428 - 0.2757 \\\\times 9.6 = 8.496$. Of course, you can put those steps together and it simplifies right down to the intercept for the second segment = $\\\\beta_0 - \\\\beta_2 k_1 = 21.7057 - 1.3760 \\\\times 9.6$.</p>\\n\\n<blockquote>\\n  <p>Can the model be reparameterized to do this in one calculation?</p>\\n</blockquote>\\n\\n<p>Well, yes, but it's probably easier in general to just compute it from the model.</p>\\n\\n<blockquote>\\n  <p>2. How to calculate the standard error of each slope of each segment?</p>\\n</blockquote>\\n\\n<p>Since the estimate is a linear combination of regression coefficients $a^\\\\top\\\\hat\\\\beta$, where $a$ consists of 1's and 0s, the variance is  $a^\\\\top\\\\text{Var}(\\\\hat\\\\beta)a$. The standard error is the square root of that sum of variance and covariance terms.</p>\\n\\n<p>e.g. in your example, the standard error of the slope of the second segment is:</p>\\n\\n<pre><code>Sb &lt;- vcov(mod)[2:3,2:3]\\nsqrt(sum(Sb))\\n</code></pre>\\n\\n<p>alternatively in matrix form:</p>\\n\\n<pre><code>Sb &lt;- vcov(mod)\\na &lt;- matrix(c(0,1,1),nr=3)\\nsqrt(t(a) %*% Sb %*% a)\\n</code></pre>\\n\\n<blockquote>\\n  <p>3. How to test whether two adjacent slopes have the same slopes (i.e. whether the breakpoint can be omitted)?</p>\\n</blockquote>\\n\\n<p>This is tested by looking at the coefficient in the table of that segment. See this line:</p>\\n\\n<pre><code>I(pmax(x - 9.6, 0))   1.3760     0.2688   5.120 8.54e-05 ***\\n</code></pre>\\n\\n<p>That's the <em>change in slope</em> at 9.6. If that change is different from 0, the two slopes aren't the same. So the p-value for a test that the second segment has the same slope as the first is right at the end of that line.</p>\\n\",\n",
       "  'CommentCount': '5',\n",
       "  'CreationDate': '2013-06-15T07:53:13.860',\n",
       "  'Id': '61811',\n",
       "  'LastActivityDate': '2013-06-15T09:09:33.037',\n",
       "  'LastEditDate': '2013-06-15T09:09:33.037',\n",
       "  'LastEditorUserId': '805',\n",
       "  'OwnerUserId': '805',\n",
       "  'ParentId': '61805',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '6'},\n",
       " {'Body': \"<p>My naive approach, which answers question 1:</p>\\n\\n<pre><code>mod2 &lt;- lm(y~I((x&lt;9.6)*x)+as.numeric((x&lt;9.6))+\\n             I((x&gt;=9.6)*x)+as.numeric((x&gt;=9.6))-1)\\nsummary(mod2)\\n\\n#                        Estimate Std. Error t value Pr(&gt;|t|)    \\n# I((x &lt; 9.6) * x)        -1.1040     0.2328  -4.743 0.000221 ***\\n# as.numeric((x &lt; 9.6))   21.7188     1.3099  16.580 1.69e-11 ***\\n# I((x &gt;= 9.6) * x)        0.2731     0.1560   1.751 0.099144 .  \\n# as.numeric((x &gt;= 9.6))   8.5442     2.6790   3.189 0.005704 ** \\n</code></pre>\\n\\n<p>But I'm not sure if the statistics (in particular degrees of freedom) are done correctly, if you do it this way.</p>\\n\",\n",
       "  'CommentCount': '1',\n",
       "  'CreationDate': '2013-06-15T08:02:29.993',\n",
       "  'Id': '61812',\n",
       "  'LastActivityDate': '2013-06-15T08:02:29.993',\n",
       "  'OwnerUserId': '11849',\n",
       "  'ParentId': '61805',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '4'},\n",
       " {'Body': '<p>SPSS usually provides univariate tests of such a main effect on each variable all the way down in the output (“Tests of Between-Subjects Effects”), even for doubly multivariate designs. So, barring any particular problem in the way you specified the model, they should be there. There are also multivariate tests of between-subject factors (why are you ignoring them?)</p>\\n\\n<p>Furthermore, from your description at the end, I am not sure that you should be looking at a main effect. It could be that the interaction in fact addresses your research question, see my answer to <a href=\"http://stats.stackexchange.com/questions/57778/experiment-with-two-groups-pre-and-post-treatment-assessments/57833#57833\">Experiment with two groups, pre- and post- treatment assessments</a>.</p>\\n\\n<p>There is also a lot of relevant material on this site that you might want to read, in particular <a href=\"http://stats.stackexchange.com/questions/3466/best-practice-when-analysing-pre-post-treatment-control-designs\">Best practice when analysing pre-post treatment-control designs</a></p>\\n',\n",
       "  'CommentCount': '4',\n",
       "  'CreationDate': '2013-06-15T09:09:56.953',\n",
       "  'Id': '61813',\n",
       "  'LastActivityDate': '2013-06-15T09:09:56.953',\n",
       "  'OwnerUserId': '6029',\n",
       "  'ParentId': '61808',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '2'},\n",
       " {'Body': '<p>Briefly, my answer is \"yes\".</p>\\n\\n<p>The result of any LDA inference algorithm is $\\\\theta_{d,t}$ and $\\\\phi_{t,w}$, distribution of topics in each document and distribution of terms in each topics. Given these distributions, one can obtain estimate for $p(z|d,w)$, conditional probability of a topic $z$ for word $w$ in document $d$:\\n$$\\np(z|d,w)=\\\\frac{p(z,d,w)}{\\\\sum\\\\limits_{s=1}^K p(s,d,w)}=\\\\frac{\\\\theta_{d,z} \\\\phi_{z,w}}{\\\\sum\\\\limits_{s=1}^{K} \\\\theta_{d, s} \\\\phi_{s, w}}\\n$$\\nFurther utilizing of this information can be, for example, assigning the single most probable topic for a word. But it\\'s not obligatory.</p>\\n',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T09:36:39.560',\n",
       "  'Id': '61814',\n",
       "  'LastActivityDate': '2013-06-15T09:36:39.560',\n",
       "  'OwnerUserId': '26924',\n",
       "  'ParentId': '61648',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '2'},\n",
       " {'AcceptedAnswerId': '61831',\n",
       "  'AnswerCount': '2',\n",
       "  'Body': '<p>$Y_{1...n}\\\\sim \\\\operatorname{Bin}(1,p)$, iid,  and I need to find an unbiased estimator for $\\\\theta=\\\\operatorname{var}(y_i)$.</p>\\n\\n<p>I did some calculations and I think that the answer is $p(1-p)-\\\\frac{p(1-p)}{n}$</p>\\n\\n<ul>\\n<li>Is this correct?</li>\\n<li>If not, how can I find an unbiased estimator?</li>\\n</ul>\\n',\n",
       "  'CommentCount': '2',\n",
       "  'CreationDate': '2013-06-15T10:08:12.600',\n",
       "  'FavoriteCount': '3',\n",
       "  'Id': '61815',\n",
       "  'LastActivityDate': '2013-06-15T19:17:27.730',\n",
       "  'LastEditDate': '2013-06-15T16:33:30.413',\n",
       "  'LastEditorUserId': '10849',\n",
       "  'OwnerUserId': '23956',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '3',\n",
       "  'Tags': '<self-study><estimation><unbiased-estimator>',\n",
       "  'Title': 'Unbiased estimator of variance of binomial variable',\n",
       "  'ViewCount': '1805'},\n",
       " {'Body': \"<p>You can probably look at your data in another way. Start with a t-test of the difference in average response time between the groups (that would be equivalent to a test that the Pearson – not Spearman – correlation between group membership and average time is 0). From there, you can easily add other variables in the model (i.e. turn it into an ANOVA or linear regression), consider transformations, rank-based statistics or generalized linear model if needed, etc.</p>\\n\\n<p>Because of the specifics of this type of experiments, it's also standard practice to analyze only successful trials and to exclude large response times before computing the means (there are better and more principled ways to deal with this problem but you definitely need to do something about it).</p>\\n\\n<p>It could also be more appropriate to analyze the response time to individual trials directly, using a multilevel model (see the literature on the “language-as-fixed-effect fallacy” in psycholinguistics).</p>\\n\\n<p>@AdamO is right, thought, doing all this after the fact is at best suggestive. If you fiddle with the model until you get something you like, <em>p</em>-values become meaningless. Also, you might have heard of the mounting debate on reproducibility within psychology. I personally think that the ease with which we explain away unexpected results through ancillary variables or details of the procedure is part of the problem. The effect might not be what you expected but a second look at the literature might reveal that it was not as strong as it first seems. If that's the case, the “disappearance” of the effect really does not need any explanation.</p>\\n\",\n",
       "  'CommentCount': '3',\n",
       "  'CreationDate': '2013-06-15T10:10:16.437',\n",
       "  'Id': '61816',\n",
       "  'LastActivityDate': '2013-06-15T10:10:16.437',\n",
       "  'OwnerUserId': '6029',\n",
       "  'ParentId': '61779',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '1'},\n",
       " {'Body': \"<p>The modified Mathisen test (Hettmansperger &amp; McKean (1998), <em>Robust Nonparametric Statistical Methods</em>, Ch. 2.11) tests for equal medians under the null hypothesis while making no assumptions about the distributions of either sample other than that they're continuous.</p>\\n\\n<p>But do bear in mind what @Michael said&mdash;a difference in medians is usually interesting only when you think there's a location parameter shifting. More often in your situation people use the Mann&ndash;Whitney&ndash;Wilcoxon test to assess stochastic dominance: that's whether one group has a lower or higher cumulative distribution function of the response for all its values &amp; would correspond to the idea of seeing whether the toxin has a good, a bad, or not much effect. The assumption here is that the population cdfs don't cross, &amp; is usually checked by examining a plot of the sample cdfs.</p>\\n\",\n",
       "  'CommentCount': '1',\n",
       "  'CreationDate': '2013-06-15T10:11:40.500',\n",
       "  'Id': '61817',\n",
       "  'LastActivityDate': '2013-07-25T16:00:21.250',\n",
       "  'LastEditDate': '2013-07-25T16:00:21.250',\n",
       "  'LastEditorUserId': '17230',\n",
       "  'OwnerUserId': '17230',\n",
       "  'ParentId': '61786',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '0'},\n",
       " {'AnswerCount': '0',\n",
       "  'Body': \"<p>For least squares with one predictor:</p>\\n\\n<p>$y = \\\\beta x + \\\\epsilon$</p>\\n\\n<p>If $x$ and $y$ are standardised prior to fitting (i.e. $\\\\sim N(0,1)$), then:</p>\\n\\n<ul>\\n<li>$\\\\beta$ is the same as the Pearson correlation coefficient, $r$.</li>\\n<li>$\\\\beta$ is the same in the reflected regression: $x = \\\\beta y + \\\\epsilon$</li>\\n</ul>\\n\\n<p>For generalised least squares (GLS), does the same apply? I.e. if I standardise my data, can I obtained correlation coefficients directly from the regression coefficients?</p>\\n\\n<p>From experimenting with data, the reflected GLS leads to different $\\\\beta$ coefficients and also I'm not sure that I'm believing that the regression coefficients fit with my expected values for correlation. I know people quote GLS correlation coefficients, so I am wondering how they arrive at them and hence what they really mean?</p>\\n\",\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T11:45:11.080',\n",
       "  'FavoriteCount': '1',\n",
       "  'Id': '61818',\n",
       "  'LastActivityDate': '2013-08-21T18:01:20.857',\n",
       "  'LastEditDate': '2013-08-21T18:01:20.857',\n",
       "  'LastEditorUserId': '20836',\n",
       "  'OwnerUserId': '20836',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '7',\n",
       "  'Tags': '<regression><correlation><least-squares><generalized-least-squares>',\n",
       "  'Title': 'Generalised least squares: from regression coefficients to correlation coefficients?',\n",
       "  'ViewCount': '165'},\n",
       " {'AcceptedAnswerId': '61844',\n",
       "  'AnswerCount': '1',\n",
       "  'Body': '<p>In <a href=\"http://robjhyndman.com/papers/complex-seasonality/\" rel=\"nofollow\">De Liv\\xadera, Hyndman &amp; Snyder</a> (2011), a TBATS model (exponential smoothing state space model with Box-Cox transformation, ARMA errors, trend and seasonal components) for additive seasonality has been discussed. Can TBATS be used with multiplicative seasonality? Are there any restraints or numerical difficulties in TBATS or BATS with multiplicative seasonality?</p>\\n',\n",
       "  'CommentCount': '4',\n",
       "  'CreationDate': '2013-06-15T11:51:01.233',\n",
       "  'Id': '61819',\n",
       "  'LastActivityDate': '2013-06-15T23:43:42.857',\n",
       "  'LastEditDate': '2013-06-15T23:35:55.293',\n",
       "  'LastEditorUserId': '805',\n",
       "  'OwnerUserId': '2959',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '1',\n",
       "  'Tags': '<time-series><forecasting>',\n",
       "  'Title': 'tbats model for multiplicative seasonality',\n",
       "  'ViewCount': '595'},\n",
       " {'Body': '<p>Yes. The union of two independent random samples will also be a random sample. But why count the intersection only once? Using both samples together is also a random sample and has a larger sample size. </p>\\n',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T12:26:26.427',\n",
       "  'Id': '61820',\n",
       "  'LastActivityDate': '2013-06-15T12:43:31.607',\n",
       "  'LastEditDate': '2013-06-15T12:43:31.607',\n",
       "  'LastEditorUserId': '24073',\n",
       "  'OwnerUserId': '24073',\n",
       "  'ParentId': '61793',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '1'},\n",
       " {'AcceptedAnswerId': '61825',\n",
       "  'AnswerCount': '1',\n",
       "  'Body': '<p>I have two random variables following normal distributions,\\n$X\\\\sim N(\\\\mu_{x},\\\\sigma_{x})$\\nand\\n$Y\\\\sim N(\\\\mu_{y},\\\\sigma_{y})$</p>\\n\\n<p>I know the covariance is $\\\\operatorname{Cov}(X,Y) = c$.</p>\\n\\n<p>And I believe (correct me if I\\'m wrong) that I can estimate the expected value of $Y$ given $X=\\\\tau$ as follows:</p>\\n\\n<p>$E(Y|X=\\\\tau) = \\\\rho \\\\frac{\\\\sigma_{y}}{\\\\sigma_{x}}(\\\\tau -\\\\mu_{x})+\\\\mu_{y}$</p>\\n\\n<p>where\\n$\\\\rho = c/(\\\\sigma_{x}\\\\sigma_{y})$.</p>\\n\\n<p>I believe this is a decent estimate. But now I want some way of estimating our \"certainty\" that our calculated value is correct. Obviously for $|\\\\rho| \\\\sim 1$ we can be very confident. For $\\\\rho \\\\sim 0.5$ a little bit confident. And for $\\\\rho \\\\sim 0$ I suppose our confidence isn\\'t changed at all. i.e. since no correlation exists we have not changed any ideas about how confident we are about our expected $Y$ value.</p>\\n\\n<p>How can I express this numerically? </p>\\n',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T12:50:49.143',\n",
       "  'Id': '61821',\n",
       "  'LastActivityDate': '2013-06-15T19:34:57.187',\n",
       "  'LastEditDate': '2013-06-15T19:34:57.187',\n",
       "  'LastEditorUserId': '17230',\n",
       "  'OwnerUserId': '26851',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '1',\n",
       "  'Tags': '<self-study><normal-distribution><conditional-expectation><confidence>',\n",
       "  'Title': 'Bivariate conditional probability confidence',\n",
       "  'ViewCount': '69'},\n",
       " {'AcceptedAnswerId': '61823',\n",
       "  'AnswerCount': '2',\n",
       "  'Body': '<p>If $K_1, \\\\dots, K_n$ are i.i.d. Poisson distributions with parameter $\\\\beta$ I have worked out that the maximum likelihood estimate is $$\\\\hat\\\\beta (k_1, \\\\dots, k_n) = \\\\frac{1}{n} \\\\sum_{i=1}^n k_i$$ for data $k_1, \\\\dots, k_n$. Therefore we can define the corresponding estimator  $$T = \\\\frac{1}{n} \\\\sum_{i=1}^n K_i .$$\\nMy question is how would you work out the variance of this estimator? </p>\\n\\n<p>In particular, as each $K_i$ follows a Poisson distribution with parameter $\\\\beta$ I know, from the properties of the Poisson, that the distribution $\\\\sum_{i=1}^n K_i$ will follow a Poisson distribution with parameter $n \\\\beta$, but what is the distribution of $T$? </p>\\n',\n",
       "  'CommentCount': '1',\n",
       "  'CreationDate': '2013-06-15T13:08:38.213',\n",
       "  'Id': '61822',\n",
       "  'LastActivityDate': '2013-06-15T14:59:35.950',\n",
       "  'LastEditDate': '2013-06-15T14:43:40.467',\n",
       "  'LastEditorUserId': '22047',\n",
       "  'OwnerUserId': '26848',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '3',\n",
       "  'Tags': '<variance><maximum-likelihood><poisson>',\n",
       "  'Title': 'Finding the variance of the estimator for the maximum likelihood for the Poisson distribution',\n",
       "  'ViewCount': '304'},\n",
       " {'Body': '<p>$T$ is distributed... as a Poisson variable scaled by $n$. Hence the variance of $T$ is $1/n^2 \\\\times n\\\\beta$.</p>\\n',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T13:22:22.913',\n",
       "  'Id': '61823',\n",
       "  'LastActivityDate': '2013-06-15T13:22:22.913',\n",
       "  'OwnerUserId': '892',\n",
       "  'ParentId': '61822',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '3'},\n",
       " {'AnswerCount': '2',\n",
       "  'Body': '<p>I use a standard GARCH model:\\n\\\\begin{align}\\nr_t=\\\\sigma_t\\\\epsilon_t\\\\\\\\\\n\\\\sigma^2_t=\\\\gamma_0 + \\\\gamma_1 a_{t-1}^2 + \\\\delta_1 \\\\sigma^2_{t-1}\\n\\\\end{align}</p>\\n\\n<p>and $a_t=\\\\sigma_t\\\\,\\\\epsilon_t$</p>\\n\\n<p>I have different estimates of the coefficients and I need to interpret them. Therefore I am wondering about a nice interpretation, so what does $\\\\gamma_0$,$\\\\gamma_1$ and $\\\\delta_1$ represent?</p>\\n\\n<p>I see that $\\\\gamma_0$ is something like a constant part. So it represents kind of an \"ambient volatility\". The $\\\\gamma_1$ represents the adjustment to past shocks. Also, the $\\\\delta_1$ is not very intuitively for me: It represents the adjustment to pas volatility. But I would like to have a better and more comprehensive interpretation of these parameters.</p>\\n\\n<p>So can anyone give me a good explanation of what those parameters represent and how a change in the parameters could be explained (so what does it mean if e.g. the $\\\\gamma_1$ increases?).</p>\\n\\n<p>Also, I looked it up in several books (e.g. in Tsay), but I could not find good information, so any literature recommendation about the interpretation of these parameters would be appreciated.</p>\\n\\n<p>Edit: I would be also interested in how to interpret the persistence. \\nSo what is exactly persistence?</p>\\n\\n<p>In some books I read, that the persistence of a GARCH(1,1) is $\\\\gamma_1+\\\\delta_1$, but e.g. in the book by <a href=\"http://www.amazon.de/Market-Risk-Analysis-Financial-Instruments/dp/B009NNW5A8\" rel=\"nofollow\">Carol Alexander</a> on page 283 he talks about only the $\\\\beta$ parameter (my $\\\\delta_1$) being the persistence parameter. So is there a difference between persistence in volatility ($\\\\sigma_t$) and persistence in shocks ($a_t$)?\\n<img src=\"http://i.stack.imgur.com/2t9hd.png\" alt=\"vo\"></p>\\n',\n",
       "  'CommentCount': '4',\n",
       "  'CreationDate': '2013-06-15T14:32:51.140',\n",
       "  'FavoriteCount': '10',\n",
       "  'Id': '61824',\n",
       "  'LastActivityDate': '2014-06-10T17:49:29.947',\n",
       "  'LastEditDate': '2013-06-21T03:43:46.663',\n",
       "  'LastEditorUserId': '805',\n",
       "  'OwnerUserId': '21998',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '8',\n",
       "  'Tags': '<interpretation><garch>',\n",
       "  'Title': 'How to interpret GARCH parameters?',\n",
       "  'ViewCount': '3529'},\n",
       " {'Body': '<p>One measure of how \"accurate\" the estimate is is the variance (often called the residual\\nvariance) of the estimate which in this instance is $\\\\sigma_y^2(1-\\\\rho^2)$: the closer\\nthe residual variance to $\\\\sigma_y^2$, the weaker our confidence that the\\nestimate is <em>significantly</em> better than discarding the observed value $\\\\tau$ of $X$\\nand just using $\\\\mu_y$ as the estimate of $Y$.  So our confidence in accuracy \\nis a function of $\\\\rho^2$, not $\\\\rho$,\\nwhich is why $\\\\rho=0.5$ does not make us feel very confident, and even\\n$\\\\rho = \\\\frac{1}{\\\\sqrt{2}}\\\\approx 0.707\\\\ldots$ feels more like an even bet than a strong\\nvote of confidence.</p>\\n',\n",
       "  'CommentCount': '2',\n",
       "  'CreationDate': '2013-06-15T14:35:28.323',\n",
       "  'Id': '61825',\n",
       "  'LastActivityDate': '2013-06-15T14:41:14.177',\n",
       "  'LastEditDate': '2013-06-15T14:41:14.177',\n",
       "  'LastEditorUserId': '6633',\n",
       "  'OwnerUserId': '6633',\n",
       "  'ParentId': '61821',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '1'},\n",
       " {'Body': \"<p>Remember that \\n$$\\n  \\\\mathbb{Var}\\\\left(\\\\sum_{i=1}^n a_i X_i\\\\right) = \\\\sum_{i=1}^n a_i^2\\\\,\\\\mathbb{Var}(X_i) + 2 \\\\sum_{1\\\\leq i&lt;j\\\\leq n} a_i\\\\,a_j\\\\,\\\\mathbb{Cov}(X_i X_j) \\\\, ,\\n$$\\nalways. But, if the $X_i$'s are independent, what is the value of $\\\\mathbb{Cov}(X_i X_j)$? That's all you need to answer the question.</p>\\n\",\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T14:59:35.950',\n",
       "  'Id': '61826',\n",
       "  'LastActivityDate': '2013-06-15T14:59:35.950',\n",
       "  'OwnerUserId': '9394',\n",
       "  'ParentId': '61822',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '3'},\n",
       " {'AnswerCount': '3',\n",
       "  'Body': '<p>Are there any free econometrics textbooks available online for individual use? </p>\\n\\n<p>I\\'m aware of Bruce Hansen\\'s first year PhD <a href=\"http://www.ssc.wisc.edu/~bhansen/econometrics/\" rel=\"nofollow\">Econometrics</a> textbook, but I\\'d be interested to hear if there are any other such resources.</p>\\n\\n<p>Note: \\nThis question is similar to an existing <a href=\"http://stats.stackexchange.com/questions/170/free-statistical-textbooks\">CV question</a>, but I\\'m specifically asking for references that would be useful to econometricians and students of economics. </p>\\n',\n",
       "  'CommentCount': '8',\n",
       "  'CreationDate': '2013-06-15T15:47:02.177',\n",
       "  'FavoriteCount': '3',\n",
       "  'Id': '61827',\n",
       "  'LastActivityDate': '2013-07-18T13:06:09.950',\n",
       "  'LastEditDate': '2013-06-21T12:03:21.713',\n",
       "  'LastEditorUserId': '24617',\n",
       "  'OwnerUserId': '24617',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '4',\n",
       "  'Tags': '<econometrics><references>',\n",
       "  'Title': 'Free econometrics textbooks',\n",
       "  'ViewCount': '381'},\n",
       " {'Body': '<p>This thread is a little old, but it appears that Wikipedia may have changed its definition and if it\\'s accurate, it explains it more clearly for me:</p>\\n\\n<blockquote>\\n  <p>An \"estimator\" or \"point estimate\" is a statistic (that is, a function\\n  of the data) that is used to infer the value of an unknown parameter\\n  in a statistical model.</p>\\n</blockquote>\\n\\n<p>So a statistic refers to the data itself and a calculation with that data. While an estimator refers to a parameter in a model.</p>\\n\\n<p>If I understand it correctly, then, the mean is a statistic and may also be an estimator. The mean of a sample is a statistic (sum of the sample divided by the sample size). The mean of a sample is also an estimator of the mean of the population, assuming it\\'s normally distributed.</p>\\n\\n<p>I\\'d ask @whuber and others who really know this stuff if the (new?) Wikipedia quote is accurate.</p>\\n',\n",
       "  'CommentCount': '1',\n",
       "  'CreationDate': '2013-06-15T15:49:45.290',\n",
       "  'Id': '61828',\n",
       "  'LastActivityDate': '2013-06-15T15:49:45.290',\n",
       "  'OwnerUserId': '1764',\n",
       "  'ParentId': '47728',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '2'},\n",
       " {'AnswerCount': '2',\n",
       "  'Body': '<p>I have values for <code>True Positive (TP)</code> and <code>False Negative (FN)</code> as follows:</p>\\n\\n<pre><code>TP = 0.25\\nFN = 0.75\\n</code></pre>\\n\\n<p>From those values, can we calculate <code>False Positive (FP)</code> and <code>True Negative (TN)</code>?</p>\\n',\n",
       "  'CommentCount': '1',\n",
       "  'CreationDate': '2013-06-15T15:59:27.653',\n",
       "  'FavoriteCount': '2',\n",
       "  'Id': '61829',\n",
       "  'LastActivityDate': '2014-03-08T10:50:56.410',\n",
       "  'LastEditDate': '2013-06-16T14:05:07.907',\n",
       "  'LastEditorUserId': '686',\n",
       "  'OwnerUserId': '5907',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '8',\n",
       "  'Tags': '<roc><sensitivity><specificity>',\n",
       "  'Title': 'Relation between true positive, false positive, false negative and true negative',\n",
       "  'ViewCount': '15566'},\n",
       " {'Body': '<p><strong>EDIT:</strong> see the answer of Gaël Laurans, which is more accurate.</p>\\n\\n<p>If your true positive rate is 0.25 it means that every time you call a positive, you have a probability of 0.75 of being wrong. This is your false positive rate. Similarly, every time you call a negative, you have a probability of 0.25 of being right, which is your true negative rate.</p>\\n',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T16:27:02.417',\n",
       "  'Id': '61830',\n",
       "  'LastActivityDate': '2013-06-15T19:31:30.763',\n",
       "  'LastEditDate': '2013-06-15T19:31:30.763',\n",
       "  'LastEditorUserId': '10849',\n",
       "  'OwnerUserId': '10849',\n",
       "  'ParentId': '61829',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '8'},\n",
       " {'Body': '<p>This answer cannot be correct. An estimator cannot depend on the values of the parameters: since they are unknown it would mean that you cannot compute the estimate.</p>\\n\\n<p>An unbiased estimator of the variance for <em>every</em> distribution (with finite second moment) is</p>\\n\\n<p>$$ S^2 = \\\\frac{1}{n-1}\\\\sum_{i=1}^n (y_i - \\\\bar{y})^2.$$</p>\\n\\n<p>By expanding the square and using the definition of the average $\\\\bar{y}$, you can see that </p>\\n\\n<p>$$ S^2 = \\\\frac{1}{n} \\\\sum_{i=1}^n y_i^2 - \\\\frac{2}{n(n-1)}\\\\sum_{i\\\\neq j}y_iy_j,$$</p>\\n\\n<p>so if the variables are IID, </p>\\n\\n<p>$$E(S^2) = \\\\frac{1}{n} nE(y_j^2) - \\\\frac{2}{n(n-1)} \\\\frac{n(n-1)}{2} E(y_j)^2. $$</p>\\n\\n<p>As you see we do not need the hypothesis that the variables have a binomial distribution (except implicitly in the fact that the variance exists) in order to derive this estimator.</p>\\n',\n",
       "  'CommentCount': '3',\n",
       "  'CreationDate': '2013-06-15T17:02:22.397',\n",
       "  'Id': '61831',\n",
       "  'LastActivityDate': '2013-06-15T17:02:22.397',\n",
       "  'OwnerUserId': '10849',\n",
       "  'ParentId': '61815',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '8'},\n",
       " {'Body': '<p><a href=\"http://en.wikipedia.org/wiki/Multicollinearity\" rel=\"nofollow\">Multicollinearity</a> refers to when predictor variables are (linearly) correlated with each other.  Although the term is sometimes used to mean <em>perfectly correlated</em> (i.e., $r=1$) only, it is more often used to simply mean <em>strongly correlated</em>.  Multicollinearity need not be manifested in <em>bivariate</em> correlations; a variable could be correlated with several other variables such that all bivariate correlations are low.  </p>\\n\\n<p>Conceptually, the existence of multicollinearity means that it is difficult to determine the role each of the correlated variables is playing.  Mathematically, it manifests in larger standard errors.  Thus, collinearity reduces statistical power.  </p>\\n\\n<p>Multicollinearity can produce counter-intuitive phenomena.  For example, when a collinear variable is added or dropped from a model, other variables can switch between significance and non-significance, and / or the sign of their relationship with the response can switch between positive and negative.  </p>\\n\\n<p>Detecting and addressing multicollinearity is an important topic in multivariable statistical modeling.  </p>\\n',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T18:06:33.907',\n",
       "  'Id': '61832',\n",
       "  'LastActivityDate': '2013-06-15T22:33:43.453',\n",
       "  'LastEditDate': '2013-06-15T22:33:43.453',\n",
       "  'LastEditorUserId': '7290',\n",
       "  'OwnerUserId': '7290',\n",
       "  'PostTypeId': '5',\n",
       "  'Score': '0'},\n",
       " {'Body': 'Multicollinearity means predictor variables are correlated with each other, making it harder to determine the role each of the correlated variables is playing. Mathematically, it means the standard errors are increased. Multicollinearity can have counter-intuitive effects.',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T18:06:33.907',\n",
       "  'Id': '61833',\n",
       "  'LastActivityDate': '2013-06-15T22:33:18.327',\n",
       "  'LastEditDate': '2013-06-15T22:33:18.327',\n",
       "  'LastEditorUserId': '7290',\n",
       "  'OwnerUserId': '7290',\n",
       "  'PostTypeId': '4',\n",
       "  'Score': '0'},\n",
       " {'AnswerCount': '2',\n",
       "  'Body': '<p>I have done a job satisfaction survey where the DV is a 7 point Likert type scale and 5 IVs with 6 point Likert-type scale and 6 IVs with 5 point Likert type scale, all ordinal. </p>\\n\\n<p>Which type of analysis is suitable for this kind of study? Between ordinal and  multinomial regression which is best suited to analyze IVs that would predict the DV?</p>\\n',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T18:35:10.130',\n",
       "  'Id': '61834',\n",
       "  'LastActivityDate': '2013-06-15T20:34:51.133',\n",
       "  'LastEditDate': '2013-06-15T18:56:22.853',\n",
       "  'LastEditorUserId': '7290',\n",
       "  'OwnerUserId': '26931',\n",
       "  'PostTypeId': '1',\n",
       "  'Score': '2',\n",
       "  'Tags': '<regression><hypothesis-testing><logistic><ordinal><multinomial>',\n",
       "  'Title': 'Which is applicable, ordinal or multinomial regression model?',\n",
       "  'ViewCount': '683'},\n",
       " {'Body': '<p>The answer mainly depends on what you want to use those models for and how well these models fit your data. So I would just start by thinking what you want to do with the results once the computer program spits them out at you. This often helps in narrowing down the options. After that, just fit the remaining models and stare at them till you understand each of the outcomes and where the differences between models come from. The latter especially often helps in determining which model is most useful for your application.</p>\\n',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T18:49:06.520',\n",
       "  'Id': '61835',\n",
       "  'LastActivityDate': '2013-06-15T18:52:08.777',\n",
       "  'LastEditDate': '2013-06-15T18:52:08.777',\n",
       "  'LastEditorUserId': '22047',\n",
       "  'OwnerUserId': '23853',\n",
       "  'ParentId': '61834',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '4'},\n",
       " {'Body': '<p>@gui11aume is right of course. An outline of a derivation specific to a $\\\\operatorname{Bin}(1,\\\\pi)$ distribution follows:</p>\\n\\n<ol>\\n<li>Find the variance in terms of $\\\\pi$ to reparameterize the probability mass function: $$\\\\theta=\\\\operatorname{Var}{Y_i}=\\\\pi(1-\\\\pi)$$</li>\\n<li>Find the maximum-likelihood estimator of $\\\\theta$: $$\\\\hat\\\\theta=\\\\frac{\\\\sum{y_i}}{n}\\\\left(1-\\\\frac{\\\\sum{y_i}}{n}\\\\right)$$</li>\\n<li>Calculate its expectation: $$\\\\newcommand{\\\\E}{\\\\operatorname{E}}\\\\E\\\\hat\\\\theta=\\\\theta\\\\cdot\\\\frac{n-1}{n}.$$ Note thankfully that the bias term is a constant.</li>\\n<li>Write the unbiased estimator: $$\\\\tilde\\\\theta=\\\\frac{\\\\hat\\\\theta}{\\\\frac{n-1}{n}}=\\\\frac{\\\\sum{y_i}}{n}\\\\left(1-\\\\frac{\\\\sum{y_i}}{n}\\\\right)\\\\cdot\\\\frac{n}{n-1}=p(1-p)\\\\cdot\\\\frac{n}{n-1}$$\\nwhere $p$ is the statistic $\\\\frac{\\\\sum{y_i}}{n}$</li>\\n</ol>\\n\\n<p>Because $\\\\sum{y}$ is sufficient &amp; complete, $\\\\tilde\\\\theta$ is not just any unbiased estimator of the population variance, but the unique minimum-variance unbiased estimator.</p>\\n',\n",
       "  'CommentCount': '0',\n",
       "  'CreationDate': '2013-06-15T18:55:15.100',\n",
       "  'Id': '61836',\n",
       "  'LastActivityDate': '2013-06-15T19:17:27.730',\n",
       "  'LastEditDate': '2013-06-15T19:17:27.730',\n",
       "  'LastEditorUserId': '10849',\n",
       "  'OwnerUserId': '17230',\n",
       "  'ParentId': '61815',\n",
       "  'PostTypeId': '2',\n",
       "  'Score': '8'}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working on RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sc.textFile('spark-stats-data/allPosts/').filter(lambda x: '<row'in x)\\\n",
    "    .map(parser_list).take(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_error(x):\n",
    "    counter = 0\n",
    "    try:\n",
    "        dict(ET.fromstring(x).attrib)\n",
    "        return counter\n",
    "        \n",
    "    except:\n",
    "        ET.ParseError\n",
    "        counter += 1\n",
    "        return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sc.textFile('spark-stats-data/allPosts/').filter(lambda x: '<row'in x)\\\n",
    "    .map(parser_error).reduce(lambda x,y: x+y)\n",
    "    # 781 bad XMl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### I'm going to check if there is a relationship between the number of times a post was favorited and the Score. I'll aggregate posts by the number of favorites, and find the average score for each number of favorites.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_list2(x):\n",
    "    try:\n",
    "        return dict(ET.fromstring(x).attrib)\n",
    "    except:\n",
    "        ET.ParseError\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile('spark-stats-data/allPosts/').filter(lambda x: '<row'in x).map(parser_list2).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109522"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.selectExpr(\"INT(FavoriteCount)\", \"DOUBLE(Score)\").groupBy('FavoriteCount').avg().fillna(0).orderBy('FavoriteCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorite_score = x.select('FavoriteCount', 'avg(Score)').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorite_score = list(zip(favorite_score['FavoriteCount'], favorite_score['avg(Score)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### I will now investigate the correlation between a user's reputation and the kind of posts they make. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User data for the reputation feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = sc.textFile('spark-stats-data/allUsers/').filter(lambda x: '<row'in x).map(parser_list2).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_pd = users.toPandas() # FOR EASIER INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>DownVotes</th>\n",
       "      <th>Id</th>\n",
       "      <th>LastAccessDate</th>\n",
       "      <th>ProfileImageUrl</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>UpVotes</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15613</th>\n",
       "      <td>2390441</td>\n",
       "      <td>2013-02-20T08:48:36.937</td>\n",
       "      <td>COOLSerdash</td>\n",
       "      <td>141</td>\n",
       "      <td>21054</td>\n",
       "      <td>2015-03-07T22:29:18.567</td>\n",
       "      <td>None</td>\n",
       "      <td>6716</td>\n",
       "      <td>8454</td>\n",
       "      <td>1027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AccountId             CreationDate  DisplayName DownVotes     Id  \\\n",
       "15613   2390441  2013-02-20T08:48:36.937  COOLSerdash       141  21054   \n",
       "\n",
       "                LastAccessDate ProfileImageUrl Reputation UpVotes Views  \n",
       "15613  2015-03-07T22:29:18.567            None       6716    8454  1027  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_pd[users_pd.Id=='21054']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "reputation = users.selectExpr('INT(Id)', 'DOUBLE(Reputation)').orderBy('Reputation', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50458"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reputation.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Reputation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>919.0</td>\n",
       "      <td>100976.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>805.0</td>\n",
       "      <td>92624.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>686.0</td>\n",
       "      <td>47334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7290.0</td>\n",
       "      <td>46907.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>930.0</td>\n",
       "      <td>32283.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>17908.0</td>\n",
       "      <td>3957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>13138.0</td>\n",
       "      <td>3821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1108.0</td>\n",
       "      <td>3805.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1679.0</td>\n",
       "      <td>3747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>11852.0</td>\n",
       "      <td>3732.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id  Reputation\n",
       "0     919.0    100976.0\n",
       "1     805.0     92624.0\n",
       "2     686.0     47334.0\n",
       "3    7290.0     46907.0\n",
       "4     930.0     32283.0\n",
       "..      ...         ...\n",
       "94  17908.0      3957.0\n",
       "95  13138.0      3821.0\n",
       "96   1108.0      3805.0\n",
       "97   1679.0      3747.0\n",
       "98  11852.0      3732.0\n",
       "\n",
       "[99 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reputation_99 = reputation.toPandas()[:99]; reputation_99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posts data for question and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile('spark-stats-data/allPosts/').filter(lambda x: '<row'in x).map(parser_list2).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_type = data.selectExpr('INT(OwnerUserId)', 'INT(PostTypeId)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|OwnerUserId|PostTypeId|\n",
      "+-----------+----------+\n",
      "|      21054|         1|\n",
      "|        805|         2|\n",
      "|      22293|         1|\n",
      "|        183|         1|\n",
      "|        805|         2|\n",
      "|      11849|         2|\n",
      "|       6029|         2|\n",
      "|      26924|         2|\n",
      "|      23956|         1|\n",
      "|       6029|         2|\n",
      "+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "post_type.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109522"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_type.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "posttype_pd = post_type.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(x):\n",
    "    if x == 1:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0    \n",
    "    \n",
    "def my_func2(x):\n",
    "    if x == 2:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "posttype_pd['questions'] = posttype_pd['PostTypeId'].apply(my_func)\n",
    "posttype_pd['answers'] = posttype_pd['PostTypeId'].apply(my_func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pd = posttype_pd.drop('PostTypeId', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pd = post_pd.rename(columns={\"OwnerUserId\":\"Id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pd = post_pd.groupby(['Id']).sum().sort_values('answers', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = reputation_99.join(post_pd, on='Id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['percent'] = final['answers'] / (final['answers'] + final['questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>919.0</td>\n",
       "      <td>100976.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1206</td>\n",
       "      <td>0.996694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>805.0</td>\n",
       "      <td>92624.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2227</td>\n",
       "      <td>0.995975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>686.0</td>\n",
       "      <td>47334.0</td>\n",
       "      <td>31</td>\n",
       "      <td>1543</td>\n",
       "      <td>0.980305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7290.0</td>\n",
       "      <td>46907.0</td>\n",
       "      <td>7</td>\n",
       "      <td>856</td>\n",
       "      <td>0.991889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>930.0</td>\n",
       "      <td>32283.0</td>\n",
       "      <td>8</td>\n",
       "      <td>430</td>\n",
       "      <td>0.981735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>17908.0</td>\n",
       "      <td>3957.0</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>13138.0</td>\n",
       "      <td>3821.0</td>\n",
       "      <td>20</td>\n",
       "      <td>127</td>\n",
       "      <td>0.863946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1108.0</td>\n",
       "      <td>3805.0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1679.0</td>\n",
       "      <td>3747.0</td>\n",
       "      <td>6</td>\n",
       "      <td>97</td>\n",
       "      <td>0.941748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>11852.0</td>\n",
       "      <td>3732.0</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id  Reputation  questions  answers   percent\n",
       "0     919.0    100976.0          4     1206  0.996694\n",
       "1     805.0     92624.0          9     2227  0.995975\n",
       "2     686.0     47334.0         31     1543  0.980305\n",
       "3    7290.0     46907.0          7      856  0.991889\n",
       "4     930.0     32283.0          8      430  0.981735\n",
       "..      ...         ...        ...      ...       ...\n",
       "94  17908.0      3957.0          0      167  1.000000\n",
       "95  13138.0      3821.0         20      127  0.863946\n",
       "96   1108.0      3805.0          1       35  0.972222\n",
       "97   1679.0      3747.0          6       97  0.941748\n",
       "98  11852.0      3732.0          0      116  1.000000\n",
       "\n",
       "[99 rows x 5 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7071.0</td>\n",
       "      <td>10045.0</td>\n",
       "      <td>30</td>\n",
       "      <td>306</td>\n",
       "      <td>0.910714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  Reputation  questions  answers   percent\n",
       "35  7071.0     10045.0         30      306  0.910714"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[final.Id==7071]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_percentage = list(zip(final.Id, final.percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9438837303189342"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final.answers) / (sum(final.answers) + sum(final.questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytup = [(-1, 0.9438837303189342)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_percentage += mytup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### We'd expect the first question a user asks to be indicative of their future behavior. I'll get in to this but first I would like to see the relationship between reputation and how long it took each person to ask their first question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = sc.textFile('spark-stats-data/allUsers/').filter(lambda x: '<row'in x).map(parser_list2).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "reputation = users.selectExpr('INT(Id)', 'DOUBLE(Reputation)', 'CreationDate').\\\n",
    "withColumn('time', F.date_format('CreationDate', \"yyyy-MM-dd HH:mm:ss\")).\\\n",
    "withColumn('time2', F.to_timestamp('time', 'yyyy-MM-dd HH:mm:ss')).\\\n",
    "orderBy('Reputation', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+-------------------+-------------------+\n",
      "|   Id|Reputation|        CreationDate|               time|              time2|\n",
      "+-----+----------+--------------------+-------------------+-------------------+\n",
      "|  919|  100976.0|2010-08-13T15:29:...|2010-08-13 15:29:47|2010-08-13 15:29:47|\n",
      "|  805|   92624.0|2010-08-07T08:40:...|2010-08-07 08:40:07|2010-08-07 08:40:07|\n",
      "|  686|   47334.0|2010-08-03T19:42:...|2010-08-03 19:42:40|2010-08-03 19:42:40|\n",
      "| 7290|   46907.0|2011-11-09T04:43:...|2011-11-09 04:43:15|2011-11-09 04:43:15|\n",
      "|  930|   32283.0|2010-08-13T20:50:...|2010-08-13 20:50:47|2010-08-13 20:50:47|\n",
      "| 4505|   27599.0|2011-05-07T13:44:...|2011-05-07 13:44:25|2011-05-07 13:44:25|\n",
      "| 4253|   25406.0|2011-04-20T12:59:...|2011-04-20 12:59:07|2011-04-20 12:59:07|\n",
      "|  183|   23610.0|2010-07-20T02:56:...|2010-07-20 02:56:34|2010-07-20 02:56:34|\n",
      "|11032|   23102.0|2012-05-02T14:04:...|2012-05-02 14:04:04|2012-05-02 14:04:04|\n",
      "|28746|   22706.0|2013-08-02T14:24:...|2013-08-02 14:24:21|2013-08-02 14:24:21|\n",
      "+-----+----------+--------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reputation.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Reputation</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>time</th>\n",
       "      <th>time2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>919.0</td>\n",
       "      <td>100976.0</td>\n",
       "      <td>2010-08-13T15:29:47.140</td>\n",
       "      <td>2010-08-13 15:29:47</td>\n",
       "      <td>2010-08-13 15:29:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>805.0</td>\n",
       "      <td>92624.0</td>\n",
       "      <td>2010-08-07T08:40:07.287</td>\n",
       "      <td>2010-08-07 08:40:07</td>\n",
       "      <td>2010-08-07 08:40:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>686.0</td>\n",
       "      <td>47334.0</td>\n",
       "      <td>2010-08-03T19:42:40.907</td>\n",
       "      <td>2010-08-03 19:42:40</td>\n",
       "      <td>2010-08-03 19:42:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7290.0</td>\n",
       "      <td>46907.0</td>\n",
       "      <td>2011-11-09T04:43:15.613</td>\n",
       "      <td>2011-11-09 04:43:15</td>\n",
       "      <td>2011-11-09 04:43:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>930.0</td>\n",
       "      <td>32283.0</td>\n",
       "      <td>2010-08-13T20:50:47.397</td>\n",
       "      <td>2010-08-13 20:50:47</td>\n",
       "      <td>2010-08-13 20:50:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50453</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50454</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50455</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50456</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50457</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50458 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  Reputation             CreationDate                 time  \\\n",
       "0       919.0    100976.0  2010-08-13T15:29:47.140  2010-08-13 15:29:47   \n",
       "1       805.0     92624.0  2010-08-07T08:40:07.287  2010-08-07 08:40:07   \n",
       "2       686.0     47334.0  2010-08-03T19:42:40.907  2010-08-03 19:42:40   \n",
       "3      7290.0     46907.0  2011-11-09T04:43:15.613  2011-11-09 04:43:15   \n",
       "4       930.0     32283.0  2010-08-13T20:50:47.397  2010-08-13 20:50:47   \n",
       "...       ...         ...                      ...                  ...   \n",
       "50453     NaN         NaN                     None                 None   \n",
       "50454     NaN         NaN                     None                 None   \n",
       "50455     NaN         NaN                     None                 None   \n",
       "50456     NaN         NaN                     None                 None   \n",
       "50457     NaN         NaN                     None                 None   \n",
       "\n",
       "                    time2  \n",
       "0     2010-08-13 15:29:47  \n",
       "1     2010-08-07 08:40:07  \n",
       "2     2010-08-03 19:42:40  \n",
       "3     2011-11-09 04:43:15  \n",
       "4     2010-08-13 20:50:47  \n",
       "...                   ...  \n",
       "50453                 NaT  \n",
       "50454                 NaT  \n",
       "50455                 NaT  \n",
       "50456                 NaT  \n",
       "50457                 NaT  \n",
       "\n",
       "[50458 rows x 5 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reputation = reputation.toPandas(); reputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## question - and creating date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile('spark-stats-data/allPosts/').filter(lambda x: '<row'in x).map(parser_list2).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AcceptedAnswerId: string (nullable = true)\n",
      " |-- AnswerCount: string (nullable = true)\n",
      " |-- Body: string (nullable = true)\n",
      " |-- CommentCount: string (nullable = true)\n",
      " |-- CreationDate: string (nullable = true)\n",
      " |-- FavoriteCount: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- LastActivityDate: string (nullable = true)\n",
      " |-- LastEditDate: string (nullable = true)\n",
      " |-- LastEditorUserId: string (nullable = true)\n",
      " |-- OwnerUserId: string (nullable = true)\n",
      " |-- PostTypeId: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ViewCount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_type = data.selectExpr('INT(OwnerUserId)', 'INT(PostTypeId)', 'CreationDate').\\\n",
    "withColumn('time', F.date_format('CreationDate', \"yyyy-MM-dd HH:mm:ss\")).\\\n",
    "withColumn('time3', F.to_timestamp('time', 'yyyy-MM-dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+-------------------+-------------------+\n",
      "|OwnerUserId|PostTypeId|        CreationDate|               time|              time3|\n",
      "+-----------+----------+--------------------+-------------------+-------------------+\n",
      "|      21054|         1|2013-06-15T06:18:...|2013-06-15 06:18:20|2013-06-15 06:18:20|\n",
      "|        805|         2|2013-06-15T06:44:...|2013-06-15 06:44:59|2013-06-15 06:44:59|\n",
      "|      22293|         1|2013-06-15T07:31:...|2013-06-15 07:31:03|2013-06-15 07:31:03|\n",
      "|        183|         1|2013-06-15T07:42:...|2013-06-15 07:42:51|2013-06-15 07:42:51|\n",
      "|        805|         2|2013-06-15T07:53:...|2013-06-15 07:53:13|2013-06-15 07:53:13|\n",
      "|      11849|         2|2013-06-15T08:02:...|2013-06-15 08:02:29|2013-06-15 08:02:29|\n",
      "|       6029|         2|2013-06-15T09:09:...|2013-06-15 09:09:56|2013-06-15 09:09:56|\n",
      "|      26924|         2|2013-06-15T09:36:...|2013-06-15 09:36:39|2013-06-15 09:36:39|\n",
      "|      23956|         1|2013-06-15T10:08:...|2013-06-15 10:08:12|2013-06-15 10:08:12|\n",
      "|       6029|         2|2013-06-15T10:10:...|2013-06-15 10:10:16|2013-06-15 10:10:16|\n",
      "+-----------+----------+--------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "post_type.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = post_type.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>time</th>\n",
       "      <th>time3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21054.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-06-15T06:18:20.483</td>\n",
       "      <td>2013-06-15 06:18:20</td>\n",
       "      <td>2013-06-15 06:18:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>805.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2013-06-15T06:44:59.437</td>\n",
       "      <td>2013-06-15 06:44:59</td>\n",
       "      <td>2013-06-15 06:44:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22293.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-06-15T07:31:03.950</td>\n",
       "      <td>2013-06-15 07:31:03</td>\n",
       "      <td>2013-06-15 07:31:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-06-15T07:42:51.720</td>\n",
       "      <td>2013-06-15 07:42:51</td>\n",
       "      <td>2013-06-15 07:42:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>805.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2013-06-15T07:53:13.860</td>\n",
       "      <td>2013-06-15 07:53:13</td>\n",
       "      <td>2013-06-15 07:53:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109517</th>\n",
       "      <td>183.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-05-17T06:33:30.290</td>\n",
       "      <td>2011-05-17 06:33:30</td>\n",
       "      <td>2011-05-17 06:33:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109518</th>\n",
       "      <td>2860.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-05-17T08:42:53.303</td>\n",
       "      <td>2011-05-17 08:42:53</td>\n",
       "      <td>2011-05-17 08:42:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109519</th>\n",
       "      <td>4496.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-05-17T09:38:43.160</td>\n",
       "      <td>2011-05-17 09:38:43</td>\n",
       "      <td>2011-05-17 09:38:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109520</th>\n",
       "      <td>4446.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-05-17T14:09:26.937</td>\n",
       "      <td>2011-05-17 14:09:26</td>\n",
       "      <td>2011-05-17 14:09:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109521</th>\n",
       "      <td>1347.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-05-17T14:16:14.383</td>\n",
       "      <td>2011-05-17 14:16:14</td>\n",
       "      <td>2011-05-17 14:16:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109522 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        OwnerUserId  PostTypeId             CreationDate                 time  \\\n",
       "0           21054.0         1.0  2013-06-15T06:18:20.483  2013-06-15 06:18:20   \n",
       "1             805.0         2.0  2013-06-15T06:44:59.437  2013-06-15 06:44:59   \n",
       "2           22293.0         1.0  2013-06-15T07:31:03.950  2013-06-15 07:31:03   \n",
       "3             183.0         1.0  2013-06-15T07:42:51.720  2013-06-15 07:42:51   \n",
       "4             805.0         2.0  2013-06-15T07:53:13.860  2013-06-15 07:53:13   \n",
       "...             ...         ...                      ...                  ...   \n",
       "109517        183.0         2.0  2011-05-17T06:33:30.290  2011-05-17 06:33:30   \n",
       "109518       2860.0         1.0  2011-05-17T08:42:53.303  2011-05-17 08:42:53   \n",
       "109519       4496.0         1.0  2011-05-17T09:38:43.160  2011-05-17 09:38:43   \n",
       "109520       4446.0         2.0  2011-05-17T14:09:26.937  2011-05-17 14:09:26   \n",
       "109521       1347.0         1.0  2011-05-17T14:16:14.383  2011-05-17 14:16:14   \n",
       "\n",
       "                     time3  \n",
       "0      2013-06-15 06:18:20  \n",
       "1      2013-06-15 06:44:59  \n",
       "2      2013-06-15 07:31:03  \n",
       "3      2013-06-15 07:42:51  \n",
       "4      2013-06-15 07:53:13  \n",
       "...                    ...  \n",
       "109517 2011-05-17 06:33:30  \n",
       "109518 2011-05-17 08:42:53  \n",
       "109519 2011-05-17 09:38:43  \n",
       "109520 2011-05-17 14:09:26  \n",
       "109521 2011-05-17 14:16:14  \n",
       "\n",
       "[109522 rows x 5 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(x):  \n",
    "    if x == 1:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "post['questions'] = post['PostTypeId'].apply(my_func) # get the question column\n",
    "post = post.drop('PostTypeId', axis=1)\n",
    "post = post.rename(columns={\"OwnerUserId\":\"Id\"})\n",
    "\n",
    "post = post[post.questions==1] # subset of questions\n",
    "post = post.drop('questions', axis=1) \n",
    "\n",
    "post = post.groupby('Id').min() # get the min date for each user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = post.drop('time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = post.drop('CreationDate', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = reputation.join(post, on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['days'] = final['time3'] - final['time2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['days'] = final['days'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('3 days 21:40:42')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['time3'][0] - final['time2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['days'] = final['days'].apply(lambda \n",
    "                                    x: int(re.findall('^\\d+', x)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['Id'] = final['Id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_question = list(zip(final.Id, final.days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### It can be interesting to think about what factors influence a user to remain active on the site over a long period of time. In order not to bias the results towards older users, I'll define a time window between 100 and 150 days after account creation. If the user has made a post in this time, I'll consider them active and well on their way to being veterans of the site; if not, they are inactive and were likely brief users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the users and creation date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = sc.textFile('spark-stats-data/allUsers/').filter(lambda x: '<row'in x).map(parser_list2).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.selectExpr('INT(Id)', 'CreationDate').\\\n",
    "withColumn('time', F.date_format('CreationDate', \"yyyy-MM-dd HH:mm:ss\")).\\\n",
    "withColumn('time2', F.to_timestamp('time', 'yyyy-MM-dd HH:mm:ss')).select('Id', 'time2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|   Id|              time2|\n",
      "+-----+-------------------+\n",
      "|70185|2015-03-02 18:42:20|\n",
      "|70186|2015-03-02 19:04:13|\n",
      "|70187|2015-03-02 19:40:16|\n",
      "|70188|2015-03-02 19:46:45|\n",
      "|70189|2015-03-02 19:56:37|\n",
      "|70190|2015-03-02 19:59:18|\n",
      "|70191|2015-03-02 20:08:27|\n",
      "|70192|2015-03-02 20:10:19|\n",
      "|70193|2015-03-02 20:41:46|\n",
      "|70194|2015-03-02 20:46:08|\n",
      "+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile('spark-stats-data/allPosts/').filter(lambda x: '<row'in x).map(parser_list2).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AcceptedAnswerId: string (nullable = true)\n",
      " |-- AnswerCount: string (nullable = true)\n",
      " |-- Body: string (nullable = true)\n",
      " |-- CommentCount: string (nullable = true)\n",
      " |-- CreationDate: string (nullable = true)\n",
      " |-- FavoriteCount: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- LastActivityDate: string (nullable = true)\n",
      " |-- LastEditDate: string (nullable = true)\n",
      " |-- LastEditorUserId: string (nullable = true)\n",
      " |-- OwnerUserId: string (nullable = true)\n",
      " |-- PostTypeId: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ViewCount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = data.selectExpr('INT(OwnerUserId)', 'INT(PostTypeId)', 'CreationDate', 'INT(Score)', 'INT(AnswerCount)', 'INT(ViewCount)', 'INT(FavoriteCount)').\\\n",
    "withColumn('time', F.date_format('CreationDate', \"yyyy-MM-dd HH:mm:ss\")).\\\n",
    "withColumn('time3', F.to_timestamp('time', 'yyyy-MM-dd HH:mm:ss')).\\\n",
    "drop('CreationDate', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+-----------+---------+-------------+-------------------+\n",
      "|OwnerUserId|PostTypeId|Score|AnswerCount|ViewCount|FavoriteCount|              time3|\n",
      "+-----------+----------+-----+-----------+---------+-------------+-------------------+\n",
      "|      21054|         1|    6|          2|     1200|            1|2013-06-15 06:18:20|\n",
      "|        805|         2|    7|       null|     null|         null|2013-06-15 06:44:59|\n",
      "|      22293|         1|    2|          1|      325|         null|2013-06-15 07:31:03|\n",
      "|        183|         1|   10|          2|      602|            3|2013-06-15 07:42:51|\n",
      "|        805|         2|    6|       null|     null|         null|2013-06-15 07:53:13|\n",
      "|      11849|         2|    4|       null|     null|         null|2013-06-15 08:02:29|\n",
      "|       6029|         2|    2|       null|     null|         null|2013-06-15 09:09:56|\n",
      "|      26924|         2|    2|       null|     null|         null|2013-06-15 09:36:39|\n",
      "|      23956|         1|    3|          2|     1805|            3|2013-06-15 10:08:12|\n",
      "|       6029|         2|    1|       null|     null|         null|2013-06-15 10:10:16|\n",
      "+-----------+----------+-----+-----------+---------+-------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = main.join(users, on=(main['OwnerUserId']==users['Id']), how='left').drop('OwnerUserId', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+---------+-------------+-------------------+----+-------------------+\n",
      "|PostTypeId|Score|AnswerCount|ViewCount|FavoriteCount|              time3|  Id|              time2|\n",
      "+----------+-----+-----------+---------+-------------+-------------------+----+-------------------+\n",
      "|         1|   27|          4|     2984|           13|2010-08-09 18:03:54| 148|2010-07-19 21:55:51|\n",
      "|         1|    8|          7|     1016|            2|2010-07-27 13:49:41| 463|2010-07-27 13:39:31|\n",
      "|         1|    5|          3|     2705|            1|2010-08-09 18:10:39| 833|2010-08-09 17:58:31|\n",
      "|         1|    7|          3|     1241|            2|2010-08-31 18:56:09|1088|2010-08-25 20:48:41|\n",
      "|         1|    1|          0|       44|         null|2012-12-02 23:08:32|1342|2010-09-17 16:31:43|\n",
      "|         2|    1|       null|     null|         null|2010-09-17 16:31:46|1342|2010-09-17 16:31:43|\n",
      "|         1|   13|          6|     3772|            2|2012-01-20 15:51:34|1645|2010-10-20 14:25:23|\n",
      "|         1|    3|          1|      341|         null|2010-10-20 14:47:38|1645|2010-10-20 14:25:23|\n",
      "|         1|    1|          2|      791|         null|2012-08-03 16:01:27|1829|2010-11-05 15:06:40|\n",
      "|         2|    0|       null|     null|         null|2012-08-06 16:25:03|1829|2010-11-05 15:06:40|\n",
      "+----------+-----+-----------+---------+-------------+-------------------+----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.withColumn('days', (final['time3'].cast('long') - final['time2'].cast('long')) /(24*3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+---------+-------------+-------------------+----+-------------------+--------------------+\n",
      "|PostTypeId|Score|AnswerCount|ViewCount|FavoriteCount|              time3|  Id|              time2|                days|\n",
      "+----------+-----+-----------+---------+-------------+-------------------+----+-------------------+--------------------+\n",
      "|         1|   27|          4|     2984|           13|2010-08-09 18:03:54| 148|2010-07-19 21:55:51|   20.83892361111111|\n",
      "|         1|    8|          7|     1016|            2|2010-07-27 13:49:41| 463|2010-07-27 13:39:31|0.007060185185185185|\n",
      "|         1|    5|          3|     2705|            1|2010-08-09 18:10:39| 833|2010-08-09 17:58:31|0.008425925925925925|\n",
      "|         1|    7|          3|     1241|            2|2010-08-31 18:56:09|1088|2010-08-25 20:48:41|  5.9218518518518515|\n",
      "|         1|    1|          0|       44|         null|2012-12-02 23:08:32|1342|2010-09-17 16:31:43|   807.2755671296296|\n",
      "|         2|    1|       null|     null|         null|2010-09-17 16:31:46|1342|2010-09-17 16:31:43|3.472222222222222E-5|\n",
      "|         1|   13|          6|     3772|            2|2012-01-20 15:51:34|1645|2010-10-20 14:25:23|  457.05984953703705|\n",
      "|         1|    3|          1|      341|         null|2010-10-20 14:47:38|1645|2010-10-20 14:25:23| 0.01545138888888889|\n",
      "|         1|    1|          2|      791|         null|2012-08-03 16:01:27|1829|2010-11-05 15:06:40|   637.0380439814815|\n",
      "|         2|    0|       null|     null|         null|2012-08-06 16:25:03|1829|2010-11-05 15:06:40|   640.0544328703704|\n",
      "+----------+-----+-----------+---------+-------------+-------------------+----+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.withColumn('window', F.when((F.col(\"days\") > 100) & (F.col(\"days\") < 150),1)\\\n",
    ".otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (F.col(\"PostTypeId\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the veterans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "veterans = final.select('Id', 'PostTypeId', 'window').select('Id', 'window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "vet = veterans.groupby('Id').avg().drop('sum(Id)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "veteran_ids = vet.filter(vet['avg(window)']>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| Id|avg(Id)|         avg(window)|\n",
      "+---+-------+--------------------+\n",
      "|  5|    5.0|0.042735042735042736|\n",
      "|  8|    8.0| 0.06722689075630252|\n",
      "| 22|   22.0|               0.125|\n",
      "| 25|   25.0| 0.01282051282051282|\n",
      "| 29|   29.0| 0.16666666666666666|\n",
      "| 30|   30.0| 0.07692307692307693|\n",
      "| 52|   52.0| 0.38461538461538464|\n",
      "| 53|   53.0|                 0.5|\n",
      "| 56|   56.0| 0.10526315789473684|\n",
      "| 69|   69.0|                 0.1|\n",
      "+---+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "veteran_ids.orderBy('Id').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2027"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veteran_ids.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "veteran_ids = veteran_ids.drop('avg(window)', 'avg(Id)')\n",
    "vet_ids = veteran_ids.toPandas().squeeze()\n",
    "vet_ids = list(vet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def vets(x):\n",
    "    if x in vet_ids:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "vets_udf = udf(vets, IntegerType())\n",
    "\n",
    "final = final.withColumn('Veterans', vets_udf(final['Id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dummy(x):  \n",
    "    if x == 0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_udf = udf(dummy, IntegerType())\n",
    "final = final.withColumn('Brief', dummy_udf(final['Veterans']))\n",
    "final.select('Id', 'Brief','Veterans').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(x):\n",
    "    if x == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "myfunc = udf(myfunc, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.withColumn('questions', myfunc(final['PostTypeId']))\n",
    "questions = final.filter(final.questions==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52060"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "veterans = questions.filter(questions.Veterans==1)\n",
    "brief_users = questions.filter(questions.Brief==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13407"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veterans.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38653"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+---------+-------------+-------------------+-----+-------------------+----+------+-----+---------+\n",
      "|PostTypeId|Score|AnswerCount|ViewCount|FavoriteCount|              time3|   Id|              time2|days|senior|brief|questions|\n",
      "+----------+-----+-----------+---------+-------------+-------------------+-----+-------------------+----+------+-----+---------+\n",
      "|         1|    2|          1|     1889|            2|2012-09-21 17:58:26|11748|2012-06-04 17:06:25| 109|     1|    0|        1|\n",
      "|         1|    0|          1|       95|            0|2012-10-02 17:39:12|11748|2012-06-04 17:06:25| 120|     1|    0|        1|\n",
      "|         1|    1|          2|      604|            0|2012-10-10 16:38:59|11748|2012-06-04 17:06:25| 127|     1|    0|        1|\n",
      "|         1|    2|          0|      218|            0|2013-02-17 12:44:47|16574|2012-11-06 13:14:24| 102|     1|    0|        1|\n",
      "|         1|    0|          1|       75|            0|2013-02-18 17:40:53|16574|2012-11-06 13:14:24| 104|     1|    0|        1|\n",
      "|         1|    1|          5|      442|            1|2013-02-25 10:13:35|16574|2012-11-06 13:14:24| 110|     1|    0|        1|\n",
      "|         1|    0|          0|       38|            0|2013-02-26 06:34:22|16574|2012-11-06 13:14:24| 111|     1|    0|        1|\n",
      "|         1|    2|          1|      476|            1|2013-02-27 11:23:07|16574|2012-11-06 13:14:24| 112|     1|    0|        1|\n",
      "|         1|    1|          1|      225|            0|2013-03-20 15:20:22|16574|2012-11-06 13:14:24| 134|     1|    0|        1|\n",
      "|         1|    1|          0|      158|            0|2013-03-22 11:50:15|16574|2012-11-06 13:14:24| 135|     1|    0|        1|\n",
      "+----------+-----+-----------+---------+-------------+-------------------+-----+-------------------+----+------+-----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "veterans.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "veterans2 = veterans.select('Id', 'time3')\n",
    "veterans2 = veterans2.groupBy('Id').agg(F.min('time3').alias('time3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_users2 = brief_users.select('Id', 'time3').groupBy('Id').agg(F.min('time3').alias('time3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "vets = veterans2.join(veterans, on=('time3')).drop(veterans.Id)\\\n",
    ".select('Id', 'Score', 'AnswerCount', 'ViewCount', 'FavoriteCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1819"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+---------+-------------+\n",
      "|   Id|Score|AnswerCount|ViewCount|FavoriteCount|\n",
      "+-----+-----+-----------+---------+-------------+\n",
      "| 8078|    3|          3|     1252|         null|\n",
      "|21240|    1|          1|      675|         null|\n",
      "|27589|    4|          1|     1014|            1|\n",
      "|28957|    1|          0|       30|         null|\n",
      "|29612|    1|          1|       84|         null|\n",
      "|40513|    0|          0|       37|         null|\n",
      "|49793|    1|          0|       35|         null|\n",
      "|54005|    2|          0|       66|         null|\n",
      "| 9177|   -1|          0|      129|            0|\n",
      "|14860|    3|          2|      530|         null|\n",
      "+-----+-----+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vets.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------+--------------+------------------+\n",
      "| sum(Id)|sum(Score)|sum(AnswerCount)|sum(ViewCount)|sum(FavoriteCount)|\n",
      "+--------+----------+----------------+--------------+------------------+\n",
      "|46281460|      6444|            2361|       1684403|              2365|\n",
      "+--------+----------+----------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vets.groupby().sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.54260582737768"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vets_score = 6444/ 1819; vets_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926.004947773502"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vets_views = 1684403/ 1819; vets_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2979659153380978"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vets_answers = 2361 / 1819; vets_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3001649257833974"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vets_favorites = 2365 / 1819; vets_favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief = brief_users2.join(brief_users, on=('time3')).drop(brief_users.Id)\\\n",
    ".select('Id', 'Score', 'AnswerCount', 'ViewCount', 'FavoriteCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+---------+-------------+\n",
      "|  Id|Score|AnswerCount|ViewCount|FavoriteCount|\n",
      "+----+-----+-----------+---------+-------------+\n",
      "|1291|    7|          2|      958|            1|\n",
      "|2385|    4|          3|     1070|            1|\n",
      "|2105|    6|          1|      539|         null|\n",
      "|3891|    7|          4|     2179|            1|\n",
      "|3840|    6|          1|       83|            2|\n",
      "|1503|    1|          0|      253|         null|\n",
      "|6610|    1|          2|      447|         null|\n",
      "|7223|    1|          1|       62|         null|\n",
      "|7235|    3|          1|      857|            3|\n",
      "|7341|    3|          0|      151|         null|\n",
      "+----+-----+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brief.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21286"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------------+--------------+------------------+\n",
      "|  sum(Id)|sum(Score)|sum(AnswerCount)|sum(ViewCount)|sum(FavoriteCount)|\n",
      "+---------+----------+----------------+--------------+------------------+\n",
      "|727971557|     44748|           20670|      11800242|             12263|\n",
      "+---------+----------+----------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brief.groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1022268157474397"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief_score = 44748/ 21286; brief_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554.3663440759184"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief_views = 11800242/ 21286; brief_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9710607911303204"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief_answers = 20670 / 21286; brief_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5761063609884431"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief_favorites = 12263 / 21286; brief_favorites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Same as above, but on the full Stack Exchange data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_list2(x):\n",
    "    try:\n",
    "        x = dict(ET.fromstring(x).attrib)\n",
    "        if 'AnswerCount' not in x:\n",
    "            x['AnswerCount'] = '0'\n",
    "        if 'FavoriteCount' not in x:\n",
    "            x['FavoriteCount'] = '0'\n",
    "        if 'ViewCount' not in x:\n",
    "            x['ViewCount'] = '0'\n",
    "        return x\n",
    "\n",
    "    except:\n",
    "        ET.ParseError\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = sc.textFile('spark-stack-data/allUsers/').filter(lambda x: '<row'in x).map(parser_list2).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users = users.selectExpr('INT(Id)', 'CreationDate').\\\n",
    "withColumn('time', F.date_format('CreationDate', \"yyyy-MM-dd HH:mm:ss\")).\\\n",
    "withColumn('time2', F.to_timestamp('time', 'yyyy-MM-dd HH:mm:ss')).select('Id', 'time2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile('spark-stack-data/allPosts/').filter(lambda x: '<row'in x).map(parser_list2).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------+---------+\n",
      "|AnswerCount|FavoriteCount|ViewCount|ViewCount|\n",
      "+-----------+-------------+---------+---------+\n",
      "|          0|            0|        0|        0|\n",
      "|          0|            0|        0|        0|\n",
      "|          0|            0|        0|        0|\n",
      "|          0|            0|        0|        0|\n",
      "|          1|            0|     2429|     2429|\n",
      "|          1|            0|     5797|     5797|\n",
      "|          7|            0|      217|      217|\n",
      "|          0|            0|        0|        0|\n",
      "|          0|            0|        0|        0|\n",
      "|          3|            0|     2353|     2353|\n",
      "+-----------+-------------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('AnswerCount', 'FavoriteCount', 'ViewCount', 'ViewCount').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main = data.selectExpr('INT(OwnerUserId)', 'INT(PostTypeId)', 'CreationDate', 'INT(Score)', 'INT(AnswerCount)', 'INT(ViewCount)', 'INT(FavoriteCount)').\\\n",
    "withColumn('time', F.date_format('CreationDate', \"yyyy-MM-dd HH:mm:ss\")).\\\n",
    "withColumn('time3', F.to_timestamp('time', 'yyyy-MM-dd HH:mm:ss')).\\\n",
    "drop('CreationDate', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = main.join(users, on=(main['OwnerUserId']==users['Id']), how='left').drop('OwnerUserId', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.withColumn('days', (final['time3'].cast('long') - final['time2'].cast('long')) /(24*3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.withColumn('window', F.when((F.col(\"days\") > 100) & (F.col(\"days\") < 150),1)\\\n",
    ".otherwise(0)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "veterans = final.select('Id', 'PostTypeId', 'window').select('Id', 'window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vet = veterans.groupby('Id').avg().drop('sum(Id)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "veteran_ids = vet.filter(vet['avg(window)']>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "veteran_ids = veteran_ids.drop('avg(window)', 'avg(Id)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288285"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vet_ids = veteran_ids.toPandas().squeeze()\n",
    "vet_ids = list(vet_ids)\n",
    "len(vet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def vets(x):\n",
    "    if x in vet_ids:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "vets_udf = udf(vets, IntegerType())\n",
    "\n",
    "final = final.withColumn('Veterans', vets_udf(final['Id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy_udf = udf(dummy, IntegerType())\n",
    "#final = final.withColumn('Brief', dummy_udf(final['Veterans']))\n",
    "final = final.withColumn('Brief', F.when(F.col(\"Veterans\") == 0 ,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final = final.withColumn('questions', myfunc(final['PostTypeId']))\n",
    "final = final.withColumn('questions', F.when(F.col(\"PostTypeId\") == 1 ,1).otherwise(0))\n",
    "questions = final.filter(final.questions==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "veterans = questions.filter(questions.Veterans==1)\n",
    "brief_users = questions.filter(questions.Brief==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "veterans2 = veterans.select('Id', 'time3')\n",
    "veterans2 = veterans2.groupBy('Id').agg(F.min('time3').alias('time3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vets = veterans2.join(veterans, on=('time3')).drop(veterans.Id)\\\n",
    ".select('Id', 'Score', 'AnswerCount', 'ViewCount', 'FavoriteCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267192"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vets = vets.groupby().sum().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+--------------+------------------+\n",
      "|     sum(Id)|sum(Score)|sum(AnswerCount)|sum(ViewCount)|sum(FavoriteCount)|\n",
      "+------------+----------+----------------+--------------+------------------+\n",
      "|480101954549|    597570|          491000|     488664403|            228490|\n",
      "+------------+----------+----------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2364816311865625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 597570 / 267192 # vets_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1828.888600706608"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 488664403 /  267192  # vets_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8376298691577593"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "491000 /  267192  # vets_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8551528488876913"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "228490 / 267192   # vets_favorite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_users2 = brief_users.select('Id', 'time3').groupBy('Id').agg(F.min('time3').alias('time3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief = brief_users2.join(brief_users, on=('time3')).drop(brief_users.Id)\\\n",
    ".select('Id', 'Score', 'AnswerCount', 'ViewCount', 'FavoriteCount').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------+---------+-------------+\n",
      "|     Id|Score|AnswerCount|ViewCount|FavoriteCount|\n",
      "+-------+-----+-----------+---------+-------------+\n",
      "|2090742|   16|          4|     2406|            3|\n",
      "|    889|  203|         22|    53711|           96|\n",
      "|    263|   27|          4|     6440|           17|\n",
      "|   4271|    5|          7|     1165|            1|\n",
      "|   4118|    4|          8|     2934|            3|\n",
      "|  13277|   15|          7|     6343|           10|\n",
      "|  14322|    8|          6|    35012|            3|\n",
      "|2133351|    1|          2|      763|            0|\n",
      "|  18744|    8|          7|     4453|            1|\n",
      "|  18950|    1|          3|     1525|            0|\n",
      "+-------+-----+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brief.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------------+--------------+------------------+\n",
      "|      sum(Id)|sum(Score)|sum(AnswerCount)|sum(ViewCount)|sum(FavoriteCount)|\n",
      "+-------------+----------+----------------+--------------+------------------+\n",
      "|3204413085258|   1630897|         2184116|    1576757606|            556090|\n",
      "+-------------+----------+----------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brief.groupBy().sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456311"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1198823602925474"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1630897 / 1456311 # brief score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082.7066512578701"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1576757606 / 1456311 # brief view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4997593233862823"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2184116 / 1456311 # brief answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38184838266002247"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "556090 / 1456311 # brief favorite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Word2Vec is an alternative approach for vectorizing text data. The vectorized representations of words in the vocabulary tend to be useful for predicting other words in the document, hence the famous example \"vector('king') - vector('man') + vector('woman') ~= vector('queen')\".\n",
    "\n",
    "### I'll see how good a Word2Vec model we can train using the 'tags' of each Stack Exchange post as documents (this uses the full data set). I choose 'ggplot' as an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "#### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(x):\n",
    "    if '  <row'in x:\n",
    "        try:\n",
    "            root = ET.fromstring(x)\n",
    "        except:\n",
    "            pass\n",
    "            return (\"Empty\")\n",
    "            \n",
    "        if root != '':\n",
    "            if (\"Tags\" in root.attrib): \n",
    "                return root.attrib[\"Tags\"]\n",
    "            else:\n",
    "                return(\"Empty\")\n",
    "        else:\n",
    "            return(\"Empty\")\n",
    "    else:\n",
    "        return(\"Empty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_posts = sc.textFile('spark-stack-data/allPosts/').map(parser).filter(lambda x: x!= 'Empty')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = full_posts.map(lambda x: ([i for i in re.split(\"<|>\", x) if i], 1)).toDF(['words','num']).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               words|num|\n",
      "+--------------------+---+\n",
      "|[javascript, jque...|  1|\n",
      "|[c#, .net, encoding]|  1|\n",
      "|                [c#]|  1|\n",
      "|[java, gwt, inter...|  1|\n",
      "|[java, image, ima...|  1|\n",
      "|[php, file-io, up...|  1|\n",
      "|[jquery, asp.net-...|  1|\n",
      "|[captcha, recaptcha]|  1|\n",
      "|[eclipse, charts,...|  1|\n",
      "|    [java, xml, jsp]|  1|\n",
      "+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(inputCol=\"words\", outputCol=\"vectors\", vectorSize=100, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = w2v.fit(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.findSynonyms(\"ggplot2\", 25).toPandas();x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = list(zip(x.word, x.similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### I'd like to see if we I predict the tags of a question from its body text. Instead of predicting specific tags, I will instead try to predict if a question contains one of the top ten most common tags.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler # to create the features into one vector if needed\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_list2(x):\n",
    "    try:\n",
    "        x = dict(ET.fromstring(x).attrib)\n",
    "        if 'AnswerCount' not in x:\n",
    "            x['AnswerCount'] = '0'\n",
    "        if 'FavoriteCount' not in x:\n",
    "            x['FavoriteCount'] = '0'\n",
    "        if 'ViewCount' not in x:\n",
    "            x['ViewCount'] = '0'\n",
    "        return x\n",
    "\n",
    "    except:\n",
    "        ET.ParseError\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sc.textFile('spark-stats-data/train/').filter(lambda x: '<row'in x).map(parser_list2).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sc.textFile('spark-stats-data/test/').filter(lambda x: '<row'in x).map(parser_list2).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90046"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9954"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AcceptedAnswerId: string (nullable = true)\n",
      " |-- AnswerCount: string (nullable = true)\n",
      " |-- Body: string (nullable = true)\n",
      " |-- CommentCount: string (nullable = true)\n",
      " |-- CreationDate: string (nullable = true)\n",
      " |-- FavoriteCount: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- LastActivityDate: string (nullable = true)\n",
      " |-- OwnerUserId: string (nullable = true)\n",
      " |-- PostTypeId: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ViewCount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AcceptedAnswerId: string (nullable = true)\n",
      " |-- AnswerCount: string (nullable = true)\n",
      " |-- Body: string (nullable = true)\n",
      " |-- CommentCount: string (nullable = true)\n",
      " |-- CreationDate: string (nullable = true)\n",
      " |-- FavoriteCount: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- LastActivityDate: string (nullable = true)\n",
      " |-- LastEditDate: string (nullable = true)\n",
      " |-- LastEditorUserId: string (nullable = true)\n",
      " |-- OwnerUserId: string (nullable = true)\n",
      " |-- PostTypeId: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- ViewCount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.selectExpr('INT(Id)', 'Tags' , 'Body', 'INT(PostTypeId)').filter(train['PostTypeId']==1).\\\n",
    "drop('PostTypeId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|   Id|                Tags|                Body|\n",
      "+-----+--------------------+--------------------+\n",
      "|48396| <probability><dice>|<p>I've just play...|\n",
      "|48397|      <epidemiology>|<p>Suppose there ...|\n",
      "|48405|  <gaussian-process>|<p>I am having a ...|\n",
      "|48408|<regression><spat...|<p>From a <a href...|\n",
      "|48411|<self-study><math...|<blockquote>\n",
      "  <p...|\n",
      "|48412|<hypothesis-testi...|<p>Suppose we do ...|\n",
      "|48418|    <markov-process>|<blockquote>\n",
      "  <p...|\n",
      "|48419|<probability><pre...|<p>I'm quite an a...|\n",
      "|48425|<clustering><k-me...|<p>I'm trying to ...|\n",
      "|48427|<hypothesis-testi...|<p>I am trying to...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.selectExpr('INT(Id)', 'Body', 'INT(PostTypeId)').filter(test['PostTypeId']==1).\\\n",
    "drop('PostTypeId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting the TOP 10 used tag words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "x = train.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tags = tags.apply(lambda x: re.findall('<(.*?)>', x)); tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for row in tags:\n",
    "    res.append(row)\n",
    "\n",
    "from itertools import chain\n",
    "words = list(chain.from_iterable(res))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(words)\n",
    "top_10 = words.groupby(0)[0].count().sort_values(0, ascending=False)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = ['r', 'regression', 'time-series', 'machine-learning', 'probability',\n",
    "         'hypothesis-testing', 'distributions', 'self-study', 'logistic', 'correlation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r',\n",
       " 'regression',\n",
       " 'time-series',\n",
       " 'machine-learning',\n",
       " 'probability',\n",
       " 'hypothesis-testing',\n",
       " 'distributions',\n",
       " 'self-study',\n",
       " 'logistic',\n",
       " 'correlation']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the label of 1's and 0's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def label(x):\n",
    "    if x[0] in top_10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "label_udf = udf(label, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    return re.findall('<(.*?)>', x)\n",
    "\n",
    "clean_udf = udf(clean, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x):\n",
    "    return re.split('<p>', x)\n",
    "\n",
    "split_udf = udf(split, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn('cleaned_tags', clean_udf(train['Tags']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn('label', label_udf(train['cleaned_tags']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.withColumn('Text', split_udf(train['Body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('Tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.withColumn('Text', split_udf(test['Body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Body texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "toke = Tokenizer(inputCol='Body', outputCol='words')\n",
    "words = toke.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "sw = StopWordsRemover(inputCol=toke.getOutputCol(), outputCol='stopped')\n",
    "stop_words = sw.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing = HashingTF(inputCol= sw.getOutputCol(), outputCol='counts')\n",
    "hashes = hashing.transform(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              counts|label|\n",
      "+--------------------+-----+\n",
      "|(262144,[1076,428...|    1|\n",
      "|(262144,[4631,538...|    0|\n",
      "|(262144,[2437,383...|    0|\n",
      "|(262144,[3992,580...|    1|\n",
      "|(262144,[528,3336...|    1|\n",
      "|(262144,[3834,428...|    1|\n",
      "|(262144,[850,901,...|    0|\n",
      "|(262144,[2686,333...|    1|\n",
      "|(262144,[12974,13...|    0|\n",
      "|(262144,[5167,853...|    1|\n",
      "|(262144,[10049,20...|    0|\n",
      "|(262144,[5385,132...|    0|\n",
      "|(262144,[2564,283...|    0|\n",
      "|(262144,[1277,658...|    0|\n",
      "|(262144,[161,521,...|    1|\n",
      "|(262144,[10136,12...|    1|\n",
      "|(262144,[2066,291...|    0|\n",
      "|(262144,[16836,44...|    1|\n",
      "|(262144,[3889,703...|    1|\n",
      "|(262144,[8901,124...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashes.select('counts', 'label').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=[hashing.getOutputCol()], outputCol='features')\n",
    "vec = va.transform(hashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "toke2 = Tokenizer(inputCol='Body', outputCol='words')\n",
    "words2 = toke.transform(test)\n",
    "sw2 = StopWordsRemover(inputCol='words', outputCol='stopped')\n",
    "stop_words2 = sw2.transform(words2)\n",
    "hashing2 = HashingTF(inputCol= 'stopped', outputCol='counts')\n",
    "hashes2 = hashing2.transform(stop_words2)\n",
    "va2 = VectorAssembler(inputCols=['counts'], outputCol='features')\n",
    "vec2 = va2.transform(hashes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vec2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grid Search and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(maxIter=10, labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[toke, sw, hashing, va, logreg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_b70930419b87,\n",
       " StopWordsRemover_800aeec0db25,\n",
       " HashingTF_f1ca7bb49a5a,\n",
       " VectorAssembler_a08a7bc1aa52,\n",
       " LogisticRegression_59d313962c4c]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.getStages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder() \n",
    "    .addGrid(hashing.numFeatures, [10, 100, 500, 1000, 5000]) \n",
    "    .addGrid(logreg.regParam, [1.0, 0.5, 0.1, 0.005, 0.01]) \n",
    "    .addGrid(logreg.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "    .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|   Id|prediction|\n",
      "+-----+----------+\n",
      "|48410|       1.0|\n",
      "|48415|       1.0|\n",
      "|48420|       1.0|\n",
      "|48422|       1.0|\n",
      "|48429|       0.0|\n",
      "|48442|       1.0|\n",
      "|48496|       1.0|\n",
      "|48497|       0.0|\n",
      "|48517|       1.0|\n",
      "|48577|       0.0|\n",
      "|48582|       1.0|\n",
      "|48594|       1.0|\n",
      "|48639|       1.0|\n",
      "|48668|       1.0|\n",
      "|48724|       0.0|\n",
      "|48726|       0.0|\n",
      "|48766|       1.0|\n",
      "|48772|       0.0|\n",
      "|48833|       1.0|\n",
      "|48836|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvModel.transform(test).select('Id', 'prediction').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cvModel.transform(test).select('Id', 'prediction').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Label|prediction|\n",
      "+-----+----------+\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(vec).select('Label','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = model.transform(vec2).select('Id', 'prediction').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = list(df.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
